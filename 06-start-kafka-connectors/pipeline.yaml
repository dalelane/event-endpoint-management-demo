#
# Builds and starts Kafka Connect connectors to
#  get a stream of stock price events from the
#  AlphaVantage API into Kafka topics
#
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: pipeline-kafkaconnectors
spec:
  workspaces:
    - name: pipeline-shared-workspace
      description: data used to set up the pipeline
    - name: maven-settings
      description: configmap with maven settings.xml used for maven tasks
    - name: docker-config-json
      description: secret with an IBM entitlement key in the form of a Docker config.json file

  params:
    - name: deployment-namespace
      type: string
      description: namespace to deploy to (needs to be a namespace that an Event Streams Operator is watching)
    - name: instance-name
      type: string
      description: name of an Event Streams cluster that the connectors should produce events to

  tasks:
    # clone the git repo that contains the specifications for
    #  the connectors to be created
    - name: clone-source-code
      taskRef:
        name: git-clone
        kind: ClusterTask
      params:
        - name: url
          value: "https://github.com/dalelane/event-endpoint-management-demo"
      workspaces:
        - name: output
          workspace: pipeline-shared-workspace


    # check that we have a working Kafka cluster before trying
    #  to connect anything new to it
    - name: check-eventstreams-ready
      taskRef:
        name: wait-for-cp4i-operand
      params:
        - name: instance-name
          value: "$(params.instance-name)"
        - name: instance-type
          value: "eventstreams"
        - name: instance-namespace
          value: "$(params.deployment-namespace)"


    # create the topics that the Connectors will send
    #  change events to
    - name: create-connect-topics
      taskRef:
        name: create-resource
      params:
        - name: resource-yaml
          value: "06-start-kafka-connectors/resources/kafka-topics/"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        # need a working Event Streams to create topics
        - check-eventstreams-ready
        # need the git repo which contains the topic specs
        - clone-source-code
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace


    # copy the API keys that the connectors will use
    - name: get-alphavantage-apikey
      taskRef:
        name: copy-secret
      params:
        - name: source-secret-name
          value: "alphavantage"
        - name: source-namespace
          value: "pipeline-credentials"
        - name: target-secret-name
          value: "alphavantage"
        - name: target-namespace
          value: "$(params.deployment-namespace)"
    - name: get-twitter-apikey
      taskRef:
        name: copy-secret
      params:
        - name: source-secret-name
          value: "twitter"
        - name: source-namespace
          value: "pipeline-credentials"
        - name: target-secret-name
          value: "twitter"
        - name: target-namespace
          value: "$(params.deployment-namespace)"

    # create credentials that the connectors can use to
    #  produce events to the Kafka cluster
    - name: create-connector-credentials
      taskRef:
        name: create-kafka-credentials
      params:
        - name: credentials-username
          value: "kafka-connect-credentials"
        - name: credentials-spec
          value: "06-start-kafka-connectors/resources/kafka-connect/kafka-connect-user.yaml"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        # need a working Event Streams to create credentials
        - check-eventstreams-ready
        # need the git repo which contains the credential ACLs
        - clone-source-code
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace


    # create the Avro schemas that will be used by the
    #  datagen connectors to generate events
    - name: store-avro-schemas-for-datagen
      taskRef:
        name: create-resource
      params:
        - name: resource-yaml
          value: "06-start-kafka-connectors/resources/avro-schemas"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        # need the git repo which contains the schemas
        - clone-source-code
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace

    # some of the connectors we are using are not available
    #  as pre-built jars, so we need to clone their repos and
    #  build them now
    - name: clone-datagen-connector-source-code
      taskRef:
        name: git-clone
        kind: ClusterTask
      params:
        - name: url
          value: "https://github.com/confluentinc/kafka-connect-datagen"
        - name: deleteExisting
          value: "false"
        - name: subdirectory
          value: "kafka-connect-datagen"
      runAfter:
        - clone-source-code
      workspaces:
        - name: output
          workspace: pipeline-shared-workspace
    - name: clone-twitter-connector-source-code
      taskRef:
        name: git-clone
        kind: ClusterTask
      params:
        - name: url
          value: "https://github.com/jcustenborder/kafka-connect-twitter"
        - name: deleteExisting
          value: "false"
        - name: subdirectory
          value: "kafka-connect-twitter"
      runAfter:
        - clone-source-code
      workspaces:
        - name: output
          workspace: pipeline-shared-workspace

    - name: build-datagen-connector-from-source-code
      taskRef:
        name: maven
      runAfter:
        - clone-datagen-connector-source-code
      params:
        # goals from doc in
        # https://github.com/confluentinc/kafka-connect-datagen#build-connector-from-latest-code
        - name: GOALS
          value:
            - -DskipTests
            - clean
            - package
        # project to build
        - name: CONTEXT_DIR
          value: "kafka-connect-datagen"
        # use a more recent version of the maven image, as
        #  the default image doesn't have git (which is needed
        #  for building this project)
        - name: MAVEN_IMAGE
          value: "gcr.io/cloud-builders/mvn:latest"
      workspaces:
          # add https mirrors to avoid build failures
          #  using the default insecure repos
        - name: maven-settings
          workspace: maven-settings
          # source code from clone-connector-source-code task
        - name: source
          workspace: pipeline-shared-workspace
    - name: build-twitter-connector-from-source-code
      taskRef:
        name: maven
      runAfter:
        - clone-twitter-connector-source-code
      params:
        # goals from doc in
        # https://github.com/jcustenborder/kafka-connect-twitter#running-in-development
        - name: GOALS
          value:
            - -DskipTests
            - clean
            - package
        # project to build
        - name: CONTEXT_DIR
          value: "kafka-connect-twitter"
        # use a more recent version of the maven image, as
        #  the default image doesn't have git (which is needed
        #  for building this project)
        - name: MAVEN_IMAGE
          value: "gcr.io/cloud-builders/mvn:latest"
      workspaces:
          # add https mirrors to avoid build failures
          #  using the default insecure repos
        - name: maven-settings
          workspace: maven-settings
          # source code from clone-connector-source-code task
        - name: source
          workspace: pipeline-shared-workspace


    # build the custom Docker image for the Connect cluster
    #  pods, with the jars needed for the connectors
    - name: create-connectors-docker-image
      taskRef:
        name: create-kafka-connectors-docker-image
      params:
        - name: connect-s2i
          value: "eem-demo-connectors"
        - name: namespace
          value: "$(params.deployment-namespace)"
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace
      runAfter:
        # need the built connector jars to include in the image
        - build-datagen-connector-from-source-code
        - build-twitter-connector-from-source-code


    - name: create-connect-docker-imagestream
      taskRef:
        name: create-resource
      params:
        - name: resource-yaml
          value: "06-start-kafka-connectors/resources/kafka-connect/kafka-connect-image.yaml"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        - clone-source-code
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace


    - name: build-connector-docker-image
      taskRef:
        name: buildah-local
      runAfter:
        - create-connectors-docker-image
        - create-connect-docker-imagestream
      params:
        - name: IMAGE
          value: image-registry.openshift-image-registry.svc:5000/eventstreams/es-kafka-connect:latest
        - name: CONTEXT
          value: "06-start-kafka-connectors/resources/kafka-connect"
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace
        - name: dockerconfig
          workspace: docker-config-json


    # create the Connect cluster definition that will
    #  request the worker pods for running connectors
    - name: create-connect
      taskRef:
        name: create-resource
      params:
        - name: resource-yaml
          value: "06-start-kafka-connectors/resources/kafka-connect/kafka-connect.yaml"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        # need a working Event Streams to create a connect cluster
        - check-eventstreams-ready
        # need the API key secrets to add to the connect pod
        - get-alphavantage-apikey
        - get-twitter-apikey
        # need credentials that will be used by connects2i
        - create-connector-credentials
        # need the avro schemas to use with the datagen connectors
        - store-avro-schemas-for-datagen
        # need the git repo which contains the Connect spec
        - clone-source-code
        # need the docker image for the Connect pods
        - build-connector-docker-image
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace


    # create the Connect cluster definition that will
    #  request the worker pods for running connectors
    - name: start-connectors
      taskRef:
        name: create-resource
      params:
        - name: resource-yaml
          value: "06-start-kafka-connectors/resources/kafka-connectors/"
        - name: namespace
          value: "$(params.deployment-namespace)"
      runAfter:
        # need the topics pre-prepared for connectors to send to
        - create-connect-topics
        # need a working Connect cluster to run connectors in
        - create-connect
      workspaces:
        - name: source
          workspace: pipeline-shared-workspace
